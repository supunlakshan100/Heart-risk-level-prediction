{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation\n",
    "\n",
    "pip install -U keras-tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional (Only if errors appear)\n",
    "\n",
    "1. pip install tensorflow --upgrade --user\n",
    "2. pip install keras --upgrade --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   SEX  AGEIR   TC  HDL  SMOKE_  BPMED  DIAB_01  RISK\n",
      "0    1     48  236   66       0      1        0   1.1\n",
      "1    0     48  260   51       0      1        1   7.0\n",
      "2    0     44  187   49       1      1        0   7.0\n",
      "3    1     42  216   57       1      1        0   0.4\n",
      "4    1     56  156   42       0      1        0   2.2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dataset=pd.read_csv('cardio_dataset.csv')\n",
    "print(dataset.head())\n",
    "dataset=dataset.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=dataset[:,0:7]\n",
    "target=dataset[:,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "target=np.reshape(target, (-1,1))\n",
    "\n",
    "scaler_data = MinMaxScaler(feature_range=(0,1))\n",
    "scaler_target = MinMaxScaler()\n",
    "\n",
    "data_scaled=scaler_data.fit_transform(data)\n",
    "target_scaled=scaler_target.fit_transform(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data, train_target, test_target = train_test_split(data_scaled, target_scaled,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout\n",
    "import numpy as np\n",
    "\n",
    "from keras.optimizers import Adam,Adagrad,Adadelta\n",
    "\n",
    "def build_model(parameters):\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    for i in range(parameters.Int('num_layers', 2, 20)):\n",
    "#number of layers gana num_layers kynne.\n",
    "#2-20 athra random num ekak thorla denwa.10 unth loop eka 10 parak run wnw.\n",
    "        if(i==0):\n",
    "        #palaweni iteration ekedi units wlt random value ekak denwa.\n",
    "        #min 32 and max 512 step eka 32i ethkt 32 , 64 wge 32n 32t random value ekak gnnwa\n",
    "        #parametr choice eke randomly relu sigmoid wge euwa gnnwa.\n",
    "        #i=0 welwata wenma ekk declare krnne palaweni layer eke input dim gana denna oni nisa.\n",
    "            model.add(Dense(units=parameters.Int('#neurons layer' + str(i),min_value=32,max_value=512,step=32),\n",
    "                            activation=parameters.Choice('activation_function '+str(i),['relu','sigmoid','tanh']),input_dim=7))\n",
    "        #drop out wltath random layer ekak denwa.    \n",
    "            model.add(Dropout(parameters.Choice('drop_prob 1',[0.2,0.3,0.4,0.5])))\n",
    "        \n",
    "        else:\n",
    "            #first hiden layer ekt passe euwa.\n",
    "            model.add(Dense(units=parameters.Int('#neurons layer' + str(i),min_value=32,max_value=512,step=32),\n",
    "                                   activation=parameters.Choice('activation_function '+str(i),['relu','sigmoid','tanh'])))\n",
    "            \n",
    "            model.add(Dropout(parameters.Choice('drop_prob 2',[0.2,0.3,0.4,0.5])))\n",
    "    #last layer liner , regression problem nisa.        \n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    \n",
    "    #random optimizer 3k denwa ekak gnna.\n",
    "    model.compile(optimizer=parameters.Choice('optmz', ['adam', 'adaDelta', 'adaGrad']),loss=parameters.Choice('loss f', ['mse', 'mae']))\n",
    "    #model.compile(optimizer=Adam(parameters.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),loss='mse')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kerastuner.tuners import RandomSearch\n",
    "\n",
    "tuner = RandomSearch(build_model,objective='val_loss',max_trials=5,executions_per_trial=3,directory='project',\n",
    "                     project_name='heart-risk')\n",
    "\n",
    "#tuner means an object from tuner library\n",
    "#build_model kiyanne model ek hdna function eka\n",
    "#regression nisa val_loss e wtrk nwi mean squared error wge oni ekk use krnna puluwan\n",
    "#max_trials means randomly neural netowrk kyk thornwda kyn eka.\n",
    "#eka neural netowrk ek ki parak run krnwd kyn eka\n",
    "#neural netork save wena thena folder name eka project\n",
    "#hart rick next folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[more info](https://keras-team.github.io/keras-tuner/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 9\n",
      "num_layers (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 2, 'max_value': 20, 'step': 1, 'sampling': None}\n",
      "#neurons layer0 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': None}\n",
      "activation_function 0 (Choice)\n",
      "{'default': 'relu', 'conditions': [], 'values': ['relu', 'sigmoid', 'tanh'], 'ordered': False}\n",
      "drop_prob 1 (Choice)\n",
      "{'default': 0.2, 'conditions': [], 'values': [0.2, 0.3, 0.4, 0.5], 'ordered': True}\n",
      "#neurons layer1 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': None}\n",
      "activation_function 1 (Choice)\n",
      "{'default': 'relu', 'conditions': [], 'values': ['relu', 'sigmoid', 'tanh'], 'ordered': False}\n",
      "drop_prob 2 (Choice)\n",
      "{'default': 0.2, 'conditions': [], 'values': [0.2, 0.3, 0.4, 0.5], 'ordered': True}\n",
      "optmz (Choice)\n",
      "{'default': 'adam', 'conditions': [], 'values': ['adam', 'adaDelta', 'adaGrad'], 'ordered': False}\n",
      "loss f (Choice)\n",
      "{'default': 'mse', 'conditions': [], 'values': ['mse', 'mae'], 'ordered': False}\n"
     ]
    }
   ],
   "source": [
    "tuner.search_space_summary()\n",
    "#api train krnna yna model eke wenas wena dewak\n",
    "#num_layers wge parameters tika."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 00m 40s]\n",
      "val_loss: 0.11139084647099178\n",
      "\n",
      "Best val_loss So Far: 0.005032426056762536\n",
      "Total elapsed time: 00h 03m 09s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner.search(train_data,train_target,epochs=10,validation_data=(test_data, test_target))\n",
    "#model . fit wage ekt meke tynne tuner.search use krnwa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in project\\heart-risk\n",
      "Showing 10 best trials\n",
      "Objective(name='val_loss', direction='min')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 4\n",
      "#neurons layer0: 96\n",
      "activation_function 0: relu\n",
      "drop_prob 1: 0.2\n",
      "#neurons layer1: 288\n",
      "activation_function 1: sigmoid\n",
      "drop_prob 2: 0.4\n",
      "optmz: adam\n",
      "loss f: mse\n",
      "#neurons layer2: 320\n",
      "activation_function 2: tanh\n",
      "#neurons layer3: 512\n",
      "activation_function 3: relu\n",
      "#neurons layer4: 160\n",
      "activation_function 4: tanh\n",
      "#neurons layer5: 192\n",
      "activation_function 5: tanh\n",
      "#neurons layer6: 192\n",
      "activation_function 6: tanh\n",
      "Score: 0.005032426056762536\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 7\n",
      "#neurons layer0: 160\n",
      "activation_function 0: tanh\n",
      "drop_prob 1: 0.5\n",
      "#neurons layer1: 288\n",
      "activation_function 1: relu\n",
      "drop_prob 2: 0.2\n",
      "optmz: adaGrad\n",
      "loss f: mse\n",
      "#neurons layer2: 32\n",
      "activation_function 2: relu\n",
      "#neurons layer3: 32\n",
      "activation_function 3: relu\n",
      "#neurons layer4: 32\n",
      "activation_function 4: relu\n",
      "#neurons layer5: 32\n",
      "activation_function 5: relu\n",
      "#neurons layer6: 32\n",
      "activation_function 6: relu\n",
      "Score: 0.022363555307189625\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 9\n",
      "#neurons layer0: 320\n",
      "activation_function 0: sigmoid\n",
      "drop_prob 1: 0.4\n",
      "#neurons layer1: 512\n",
      "activation_function 1: sigmoid\n",
      "drop_prob 2: 0.5\n",
      "optmz: adam\n",
      "loss f: mse\n",
      "#neurons layer2: 448\n",
      "activation_function 2: sigmoid\n",
      "#neurons layer3: 96\n",
      "activation_function 3: relu\n",
      "#neurons layer4: 64\n",
      "activation_function 4: relu\n",
      "#neurons layer5: 256\n",
      "activation_function 5: relu\n",
      "#neurons layer6: 256\n",
      "activation_function 6: relu\n",
      "#neurons layer7: 448\n",
      "activation_function 7: sigmoid\n",
      "#neurons layer8: 128\n",
      "activation_function 8: sigmoid\n",
      "#neurons layer9: 128\n",
      "activation_function 9: sigmoid\n",
      "#neurons layer10: 384\n",
      "activation_function 10: sigmoid\n",
      "#neurons layer11: 288\n",
      "activation_function 11: relu\n",
      "#neurons layer12: 64\n",
      "activation_function 12: sigmoid\n",
      "#neurons layer13: 480\n",
      "activation_function 13: tanh\n",
      "Score: 0.02391854425271352\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 14\n",
      "#neurons layer0: 160\n",
      "activation_function 0: relu\n",
      "drop_prob 1: 0.5\n",
      "#neurons layer1: 416\n",
      "activation_function 1: tanh\n",
      "drop_prob 2: 0.5\n",
      "optmz: adam\n",
      "loss f: mae\n",
      "#neurons layer2: 416\n",
      "activation_function 2: relu\n",
      "#neurons layer3: 64\n",
      "activation_function 3: tanh\n",
      "#neurons layer4: 448\n",
      "activation_function 4: relu\n",
      "#neurons layer5: 224\n",
      "activation_function 5: sigmoid\n",
      "#neurons layer6: 256\n",
      "activation_function 6: sigmoid\n",
      "#neurons layer7: 32\n",
      "activation_function 7: relu\n",
      "#neurons layer8: 32\n",
      "activation_function 8: relu\n",
      "#neurons layer9: 32\n",
      "activation_function 9: relu\n",
      "#neurons layer10: 32\n",
      "activation_function 10: relu\n",
      "#neurons layer11: 32\n",
      "activation_function 11: relu\n",
      "#neurons layer12: 32\n",
      "activation_function 12: relu\n",
      "#neurons layer13: 32\n",
      "activation_function 13: relu\n",
      "Score: 0.11080210159222285\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 8\n",
      "#neurons layer0: 160\n",
      "activation_function 0: tanh\n",
      "drop_prob 1: 0.3\n",
      "#neurons layer1: 288\n",
      "activation_function 1: tanh\n",
      "drop_prob 2: 0.3\n",
      "optmz: adaDelta\n",
      "loss f: mae\n",
      "#neurons layer2: 32\n",
      "activation_function 2: relu\n",
      "#neurons layer3: 416\n",
      "activation_function 3: sigmoid\n",
      "#neurons layer4: 224\n",
      "activation_function 4: relu\n",
      "#neurons layer5: 224\n",
      "activation_function 5: sigmoid\n",
      "#neurons layer6: 288\n",
      "activation_function 6: tanh\n",
      "#neurons layer7: 480\n",
      "activation_function 7: relu\n",
      "#neurons layer8: 352\n",
      "activation_function 8: sigmoid\n",
      "#neurons layer9: 192\n",
      "activation_function 9: relu\n",
      "#neurons layer10: 64\n",
      "activation_function 10: sigmoid\n",
      "#neurons layer11: 512\n",
      "activation_function 11: relu\n",
      "#neurons layer12: 320\n",
      "activation_function 12: sigmoid\n",
      "#neurons layer13: 384\n",
      "activation_function 13: tanh\n",
      "Score: 0.11139084647099178\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary()\n",
    "#hodama model ekt enna oni dewal tynwa me summery eke.\n",
    "#udinma tynne hodma eka. \n",
    "#ita pahalin ilgt hodma eka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 96)                768       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 288)               27936     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 288)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 320)               92480     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 320)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               164352    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 286,049\n",
      "Trainable params: 286,049\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "best_models = tuner.get_best_models()\n",
    "print(best_models[0].summary())\n",
    "#hodama model eke wistr pahadiliwa ganna puluwan"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
